---
title: "Variance in Life Expectancy with Country’s percentage of being literate"
subtitle: "IT Semester 03"
author:
  - 'Akshara Shukla'
  - "Fontys University of Applied Sciences"
abstract: |
  This report focuses on finding the variance in Life Expectancy with different independent variables. The chosen approach is of conducing regression analysis followed with an inference to understand the relationship between the chosen variables. The deliverables include providing the reader with a result of which variable has great impact on determing the life expectancy.  

  
output: html_notebook
---
<style>
#question {
  color: blue
}
#answer {
  color: brown
}
</style>

Introduction 

This report consists of different regression analysis models formed to understand how our independent variables: alcohol consumption, smoking, body mass index and literacy percentage effects or changes our outcome variable, life expectancy of a country. The finding of data has been conducted and our data has been imported into R file from a csv file, sql database and from a html page. 


```{r}
#Laoading the required libraries into the file.
library(tidyverse)
library(ggplot2)
library(readxl)
library(rvest) 
library(skimr)
library(moderndive)

#Forming a connection with our database.
con <- DBI::dbConnect(
  odbc::odbc(),
  Driver = "SQL Server",
  Server = "MSI",
  Database = "RfinalProject", 
  Trusted_Connection = "True"
)
```


```{r}
#Importing the datasets found from different sources, CSV, SQL and HTML page.
#CSV
alcoholcons <- read.csv("alcoholconsumption.csv") %>% 
  rename(Country = ï..country,
         AlcoholPerYear = alcoloholPerYear)
smoking <- read.csv("smoking.csv") %>% 
  rename(Country = ï..country)
alcoholcons
smoking

#SQL 
literacyrate <- tbl(con,"literacyperc") %>% collect() %>% select(-year) %>% na.omit()
bmi <- tbl(con, "bmi") %>%  collect()
bmi
literacyrate

#HTML
tabletwo <- 
  read_html("lite.html") %>% 
  html_nodes("table")
tabletwo
lifeexpectancy <- 
  tabletwo[[12]] %>% 
  html_table() %>% 
  select(-`95% range`) %>% 
  mutate(LifeExpectancy = rowMeans(lifeexpectancy[,2:3]))
lifeexpectancy
```


For better readability, we are going to be joining all the tables into one vector and name it df. This df vector will be used throughout our file to accumulative valid results for our analysis. 
```{r}
df <- lifeexpectancy %>% 
  inner_join(bmi, by = "Country") %>% 
  inner_join(alcoholcons, by = "Country") %>% 
  inner_join(smoking, by = "Country") %>% 
  inner_join(literacyrate, by = c("Country" = "country"))
df
```

For having a simplified approach, we have formed four research question which involve different variables in realation to our outcome variable. 
We will be going one by one to solve the queries. 


<p id="question"> **Sub Research Question 01: What are the variables which have high correlation with life expectancy?** </p>

Our first research question focuses on understanding which independent variables has a strong correlation with our dependent variable life expectancy. Correlation coefficient is the magnitude between -1 and 1 which quantifies the relationship between two variables. For obtaining the correlation coefficient of each independent variable with life expectancy we first stored all our independent variables which are: *alcohol consumption*, *smoking rate*,  *literacyperc*, *exercise* with *LifeExpectancy* in the common data frame, "df". 

Now, for visualizing the correlation, we have used the package *psych* which consists of functions for analyzing data at multiple levels of   statistical, correlations and factor analysis. The *pairs* is used to get a matrix of the dataframe specified scatterplot.

Here, we have used pairs.panels() function is useful when the number of variables to plot are less than 6-10. It is fundamentally useful for an initial overview of the data which in our case is the correlation coefficient. 
 
```{r}
library(psych)
pairs.panels(df_correlation[1:5]) #Selecting columns 1 to 5
```

The matrix above starts with our dependent variable *Life Expectancy* and lists all our dependent variables in a slanting line. The diagonal is showing our variables which from a histogram with cyan colored bars divide the matrix into two sides. The right side shows us the different correlation values between the corresponding variables and on the left we see the the relationship graph between the variable in the horizontal vs the vertical of the matrix. The left side is out of the scope of this report, therefore we wouldn't be working too much into it.

We can visually read the rounded correlation value between our Life Expectancy and our individual independent variables and also, we can read the rounded correlation value between our independent variables. The way to read the correlation value is to correspond the variables you want to find the value of and you get correlation value between those two variables. For example, when we correspond Life Expectancy with alcoholPerDay we get -0.20 which is the value of correlation between those two variables. In a similar way, when corresponding totalSmokingRate with alcoholPerDay, we get 0.15 which is the correlation value between those two variables. 

From our matrix, we can arrange the rounded correlation values of our independent variables as, -0.20, 0.33, 0.69, 0.54. Higher the value of our correlation value, stronger is the relationship between our two variables. From our values, we can conclude that 0.69 is the highest value of all which relates to the relationship between *Life Expectancy* and *LiteracyPerc*. We can interpret that positive magnitude corresponds to strength of relationship which is positively increasing i.e., as the *LiteracyPerc* increases the value of *Life Expectancy* increases. In other words, as more and more people finish schooling and become literate, their chances of living longer are higher.

On the second place, we have 0.54, which depicts the relationship between *Life Expectancy* and *MeanBMI*. The positive value of 0.54 suggests that as the value of *MeanBMI* the value of *Life Expectancy* increases. In other words, as more people maintain good health, eat healthy nutritious food and maintain a positive approach to life, tend to live longer.  


<p id="question"> **Research Question: How does the life expectancy of different country develop with its literacy rate ?** </p>

For our first research question, we are going to be explaining the difference in life expectancy (outcome variable y) as a function of a numerical variable, literacy percentange of a country (explanatory variable x1) and a categorical variable illustrating the available countries.

```{r}
literate_model <- df %>% 
  select(Country, literacyperc, LifeExpectancy)
literate_model
```
To begin with, we selected the required columns from our merged dataframe, which are: 
  *Country*, which is our categorical variable, *explanatory variable x<sub>2<sub>* which stores the data for each country,
  *LifeExpectancy*, which is our *outcome variable y*, which is a numerical variable storing the Life expectancy of people in the available countries,
  *literacyperc*, which is our  *explanatory variable x<sub>1<sub>*, which is also a numerical variable having the percentage of people who are literate in the available countries. 

  
```{r}
summary(literate_model)
```

Now our aim is to understand the raw data values in our literate_model or to say, we'll be conducting our exploratory data analysis. By using the summary() function, we can produce a summary of our variables. As we can see, in *Country* column, we have 93 countries listed which have character datatype assigned. For *LifeExpectancy*, we can see that the minimum age people lived in those coutries was 38 and the maximum age was 81 years old, with the average person living for approximately 70 years. Moreover, for our second column of *literacyperc*, we can see that the country with the least number of people who finished their education has a literacy percentage of 35 while the highest being 100 percentage, with the average amount of literate percentage being 86 percent of people.

```{r}
literate_model %>% sample_n(size = 10)
```


An alternative way to look at the raw data values is by choosing a random sample of (size = 10 ) rows by using the slice_sample() function from the dplyr package. By selecting for random 10 rows from literate_model, we can measure the different chances of the raw data values from our dataframe.


To understand more about our model, we can use the skim() function from the skimr library. By looking at the table below, we can infer that there are no missing values and all the 93 rows are carry unique values. Additionally, for our numerical variables, *literate percentage* and *LifeExpectancy*, values are distributed equally in a boxplot manner. As for Life Expectancy the, p0 denoting the p25 denoting the 25% of the values (also called as the upper quartile) and the p75 denoting the 75% of the values (also called as the lower quartile).

```{r}
literate_model %>% 
  skim()
``` 

To view the correlation coefficient between the two variables, we use the get_correlation function of the moderndive package.
Using the model formula LifeExpectancy ~ LiteracyPerc which means we want to find the correlation coefficient with Life expectancy as the outcome variable and Literacy Perc as the explanatory variable. Since, country is our categorical variable, we cannot find it's correlation, since we can find correlation only between numerical varibles.

```{r}
literate_model %>% get_correlation(formula = LifeExpectancy ~ literacyperc)
```

The correlation value is 0.694. The positive sign indicates a positive correlation. In other words, more and more people finish their education, the value of literacy percentage for that country increases, and the value of Life Expectancy of that country would increase as well and vice versa. The coefficient of 0.694 indicated a *strongly positive correlation*. Additionally, we can assume that the regression line between our variables would be a straight line facing upwards. 


Now, after understanding the data types and the correlation between our raw data values, it is time to visually plot them and understand how they vary. To achieve that, we create a scatterplot with the Literacy Percentage along the x-axis, Life Expectancy along the y-axis and the countries variable mapped to the label to the data points by using geom_text().
Since, the number of available countries are alot, therefore to avoid overlapping in labeling the data points, we have assigned check_overlap to being TRUE.


```{r}
literate_model %>%  
  ggplot(aes(literacyperc, LifeExpectancy)) + geom_point() +   
  labs(x = "Literacy Percentage",
       y = "Life Expectancy",
       title = "Scatterplot of Literacy Percentage and Life Expectancy")  + 
  geom_text(aes(label = Country), check_overlap = TRUE)  
```

From our resulting visualization, we can infer that the literacy percentage of the countries ranges from 40 to a full 100 and the living age of the population varies from 40 years to 80 years old. It is interesting to notice there, the literacy percentage of between 80 and 100 include many countries.


Next, we will be adding a regression line to understand how our outcome variable y *Life Expectancy* changes as the explanatory variable x1 *Literacy Percentage* changes for each of our categorical variable *Country*.



```{r}
literate_model %>%  
  ggplot(aes(literacyperc, LifeExpectancy)) + geom_point() + 
  labs(x = "Literacy Percentage",
       y = "Life Expectancy",
       title = "Scatterplot of Literacy Percentage and Life Expectancy") +  
  geom_smooth(method = lm)  + 
  geom_text(aes(label = Country), check_overlap = TRUE)
```

After forming the regression line by using geom_smooth(), we can see the direction of the regression line is facing upwards which fits correctly as the correlation between the two variables was 0.69 (positive).  Hence, we can correctly say that the relationship between Life Expectancy and Literacy percentage of a country is strongly positive. In other words, as more and more people finish their schooling and become literate, they tend to live longer than the other who do not.


For understanding, how exactly the value of our outcome variable y is effected by a change in the explanatory variable x, we will compute the linear regression coefficients. We use the moderndrive package to convey this. 

```{r}
literacy_model <- lm(LifeExpectancy ~ literacyperc, data = literate_model)
literacy_model %>% 
  get_regression_table()
```

From the table above, the intercept coefficient is the bo (the value of y, our outcome variable when x is 0), while the slope coefficient for x is b1.

The equation of line for regression analysis is: 
${\widehat y} = b_0 + b_1 . x_1$

The $bo$ is the slope coefficient which is 41.128.

The $b1$ is the the y intercept coefficient which is 0.333. 

Therefore, our equation of line is ${\widehat y} = 41.129 + 0.33. LiteracyPerc$

From our equation and intercept values, we can infer the relationship between the two variables. Since, the slope b1 = 0.333 is relatively very low but positive, it means that as more people finish schooling and become literate, their chances of living increases. The value of the slope indicates the overall strength of the linear relationship which is positive.

### Standard error
The third column in the regression table is of std_error, it is the difference between the estimate and actual values between the different samples. Our std_error for Literacy Percentage is 0.036 which means that there would be 0.036 units of variation seen in its slope.

Smaller the value of std_error, more closer are the data points to the regression line and thus helping in generating a better estimate. For the std_error in our intercept term, because we have chosen a particular sample size from our total population of n = 92, we have it as 3.191 but if we again picked a different sampling size with different outcome variables, the intercept would surely change,this is due to sampling variability.


### Test statistic
The fourth column in the regression table is of statistics, it includes the t-testing method which tells us what is expected within the null hypothesis. 

The *null hypothesis* $H_0$ assumes that the slope is 0 and that there is *no true relationship between literacy percentage of a country and its life expectancy*.

The *alternative hypothesis* $H_A$ assumes that the slope is not 0, which means that as the value of literacy percentage for a country increases, the life expectancy either increases (positive slope) or decreases (negative slope).

Our t-value is 9.272 which indicated that there's a positive difference between our hypothesised value and estimated value.

### p-value
The fifth column in the regression table is of p_value, it is the probability that, IF the null hypothesis is true, sampling variations would produce an estimate that is further away from our hypothesized value. 

It tells us how likely it is for the effect we have observed of the percent of people being literate on their life expectancy value (in other words, the observed fitted slope of b1 = 0.333) to be due to chance. Our p_value is 0, which means that we can reject our null hypothesis.

### Confidence interval
The last two columns from the right are our confidence interval. The confidence level is similar to a bandwidth depicting (low and high) intervals of the 95% confidence interval for the slope between which our value lies.

Here, we are 95% confident that our slope lies in the range of 0.261 and	0.405. Also, we can see that our confidence interval doesn’t contain 0, which means that there is a significant relationship between the outcome and independent variable, thus proving our null hypothesis to be wrong.

But, if we increase our percentage to 99%, there may be a chance of 0 being included. Therefore, the chosen confidence level plays an important role in coming to a result.

Now, we carry out out residual analysis for confirming if the four conditions for inferencing for regression have been met by our model. 

First, we investigate the information on individual data points by using get_regression_points().

```{r}
literacy_analysis <- literacy_model%>% 
  get_regression_points()
literacy_analysis
```

### (1) **L**inearity of relationship

This condition states that the relationship between our independent and dependent variable is linear enough i.e., the data points are moving as along with the regression line. By, looking at our scatterplot formed above, the data points are scattered but are moving along with the regression line. Moreover, we have a positive correlation coefficient value of 0.694. Hence, we can say that the relationship between the two variables is *somewhat linear*.

```{r}
cor(literate_model$LifeExpectancy, literate_model$literacyperc)
```

### (2) **I**ndependence of the residuals

The condition of independence of the residual states that the residual between the two variables must be independent of one another. We determine this by forming the residual lag plot.

```{r}
literacy_analysis %>% 
  mutate(lag_residual = lag(residual)) %>% 
  ggplot(aes(lag_residual,residual)) + geom_point()
```

By looking at our scatter plot, because we get a random plot i.e the residuals do not have a fixed pattern. Our observation values are scattered unevenly.  Hence, we can say that the residuals are *fairly independent* of each other.


### (3) **N**ormality of the residuals
The condition of normality of the residuals states that if we form a histogram of our residuals, we should get a normal distribution with the center at 0.

```{r}
literacy_analysis %>% 
  ggplot(aes(residual)) + 
  geom_histogram(binwidth = 4, color = "white") + ggtitle("Normality of the Residuals")
```

From our histogram, we can see that the long tail is on the left side of the peak thus forming a negative skew model. The regression model makes more positive errors indicating that our residual > 0. The model is underestimates our outcome variable i.e. the life expectancy of people. Although the desired shape is of a bell shape with the data being evenly spread.

### (4) **E**quality of variance of the residuals
The condition of equality of variance of the residuals states that the distribution between the residual and the independent variable should be homoscedastic (scedastic = scatter). To determine this, we form a scatter plot and compare the residuals with the yintercept = 0.


```{r}
literacy_analysis %>% 
  ggplot(aes(literacyperc,residual)) + geom_point() + geom_hline(yintercept = 0, col = "purple", size = 1) + ggtitle("Equality of variance of the residuals")
```

In our scatter plot, we can see that the residuals do not vary much as the value of our independent variable, house age changes linearly. Thus, we can infer that our model produces an accurate result. We can say that the distribution is *homoscedastic*.


From our above analysis we can say that: 

  1. Linearity of relationship between variables: Somewhat
  2. Independence of residuals: Yes
  3. Normality of residuals: Negatively Skew
  4. Equality of variance: Yes

From the four conditions of inference on regression, the condition of linearity of relationship between variables wasn't fully implied. A way to improve this condition would be to form a log transform plot and then make our inferences. Furthermore, the condition of normality of residuals was violated as well. To improve this condition and form a bell shaped histogram, we can try to incorporate more explanatory variables in the model by conducting multiple regression. 



<p id="question"> **Sub Research Question 02: Does consuming alcohol or smoking on a regular basis affect the life expectancy and if so how?** </p>

In our third question, we are focusing on understanding how does consuming *alcohol per day* and *smoking* affects a person's Life Expectancy. 
The first step is to conduct an Exploratory Data Analysis, which we begin by looking at the raw data values.

```{r}
consumption_model <- df %>% 
  select(Country, LifeExpectancy,alcoholPerDay,totalSmokingRate) %>% 
   rename(Alcohol = alcoholPerDay,
         Tabacco = totalSmokingRate) %>% 
  pivot_longer(Alcohol:Tabacco, names_to = "Consumption", values_to = "Value") %>% 
  mutate(Consumption = as.factor(Consumption))
consumption_model
```

Here, *Country* is the list of countries,
      *LifeExpectancy* is our *outcome variable y*, which is a numerical variable. It is the average computed from the life expectancy of the male and female, 
      *Consumption* is our *explanatory variable x<sub>1<sub>*, which a factor variable in our data which is the type of consumption chosen,
      *Value* is our second *explanatory variable x<sub>2<sub>* value of the corresponding type of consumption chosen.
    
    
For determining, the proportion of each consumption type per country, we used windows functions sum(). 

```{r}
consumption_model %>% 
  group_by(Consumption) %>% 
  mutate(consumptionperc = Value/sum(Value))
```


Firstly, examining the raw data our our consumption_model. Here, we used glimpe(), to see we have 186 rows or observations with 4 columns and the data type of each column. 

```{r}
consumption_model %>% glimpse()
```

To view a small random proportion of our sample, we observed 10 random observations. Additionally, checked for the available levels for our Consumption type, which are Alcohol and Smoking.

```{r}
consumption_model %>% sample_n(size = 10)
consumption_model$Consumption %>% levels()
```


```{r}
consumption_model %>% 
  summarise(min(LifeExpectancy), max(LifeExpectancy), mean(LifeExpectancy),
            min(Value), max(Value), mean(Value))

consumption_model %>% 
  get_correlation(formula = LifeExpectancy ~ Value)
```

For computing the statistical analysis, we found out that the value of Life Expectancy ranges from minimum 38 years to maximum 81 years, with the average life expectancy of our group of people being 70 years old. Furthermore, our Value, which is our associated consumption type value ranges from 2.1 to 70.1 with the average consumption rate being 28.89. 

Our correlation coefficient value is 0.0043, which is very low indicating very low relationship magnitude between the two variables. The positive sign indicates negative correlation. In other words, as the value of consuming alcohol and tabacoo regularly increases, the value of life expectancy for a person will increase and vice versa. Thus, we can say that the relationship between our variables is *weakly positive*.

We cannot find the correlation coefficient w.r.t. the categorical variable as the get_correlation function is defined only for numerical variables.


Nextly, we'll be forming a scatterplot for better visualization of our data points with Consumption Rate on x- axis and Life Expectancy on y-axis and the consumption type stored in the color aesthetic.

```{r}
consumption_model %>% 
  ggplot(aes(Value, LifeExpectancy, color = Consumption)) +
  geom_point() + 
  labs(x = "Consumption Rate",
       y = "Life Expectancy",
       color = "Consumption",
       title = "Scatterplot of relationship of Alcohol & Tabacco consumption and Life Expectancy") 
```

From our scatterplot we can infer that most of the life expectancy age values lies between 40 and 80 while for most people consumed alcohol and tabacco between 0 being the least and 60 being the maximum value. In general, we can say as more people consume alcohol or continue to smoke on a daily basis, their life expectancy decreases when compared to the people who consumed less lived longer with highest data point fairly visible above 80 for life expectancy. There can be seen overlapping of some of the data points in our plot which suggests some interaction between our independent and dependent variables.  To infer, if there exists some overlapping between the points, we will use geom_jitter(). 

```{r}
consumption_model %>% 
  ggplot(aes(Value, LifeExpectancy, color = Consumption)) +
  geom_jitter() + 
  labs(x = "Consumption Rate",
       y = "Life Expectancy",
       color = "Consumption",
       title = "Scatterplot of relationship of Alcohol & Tabacco consumption and Life Expectancy")
```

By using geom_jitter(), the number of data points didn't experience any change. To prove this, we can see there exists approximate 6 data points at consumption rate being 60 and above in both the graphs. 

Next, we will add a regression line

```{r}
model_interaction <- consumption_model %>% 
  ggplot(aes(Value, LifeExpectancy, color = Consumption)) +
  geom_point() + 
  labs(x = "Consumption Rate",
       y = "Life Expectancy",
       color = "Consumption",
       title = "Scatterplot of relationship of Alcohol & Tabacco consumption and Life Expectancy") +
  geom_smooth(method = lm, se = FALSE)
model_interaction
```


There are two regression lines, one for each type of consumption, the green representing people smoking and the red illustrating people consuming alcohol. Also, since both our lines are intersecting each other, we can infer that this is an interaction model. However,it is interesting to notice that the regression line for the people consuming alcohol have decreasing life expectancy but for the people consuming tobacco, or people who smoke, tend to live longer as the green regression line is having a straight increasing direction. This is interesting and worth investigating. 


Nextly, we'll be forming equation of each regression lines according to the interaction model. 

```{r}
model_interaction <- lm(LifeExpectancy ~ Value * Consumption, data = consumption_model)
model_interaction %>% 
  get_regression_table()
```

In the estimate column of the above table are the intercepts for the baseline for comparison for the "alcohol" consumption which is $b0$ = 74.040 and $b1$ -0.115 being the slope for consumption of only alcohol. 

The equation of the regression line for the interaction sloped model is
${\widehat y} = b_0 + b_1 . x_1 + b_2 . x_2$

In our case, our equation of regression line is
${\widehat {LifeExpectancy}} = 74.040 - 0.115 . Value$

For the people who consumed tobacco, their equation of regression line is:

Here, the consumption tobacco corresponds to the offset in the intercept for the consumption type being tobacco which is -11.165. The value 0.429 for Value:ConsumptionTabacco gives us the offset in slope for the consumption type tabacco.
${\widehat {LifeExpectancy}} = 74.040 - 0.115 . Value - 11.165 + 0.429.value$
${\widehat {LifeExpectancy}} = 62.875 + 0.314 . Value$

Here, our $b0$ = 62.875 and $b1$ = 0.314

For the people who consumed alcohol daily, their equation of regression line is
${\widehat {LifeExpectancy}} = 74.040 - 0.115 . Value$

From our obtained values of b0 and b1, we can say that the value of life expectancy is most affected by the people who consumed tabacoo or who were regular smokers. This is because, the value of b1 for tabacco is 0.314 which is higher than that value for alcohol consumers. 

### Standard error
 Our std_error for our value is 0.058 which means that there would a 0.058 unit of variation in the slope of value. While for our ComsumptionTabacco, a 3.176 unit of variation would be seen. Smaller the value of std_error, more closer are the data points to the regression line and thus helping in generating a better estimate. 

For the std_error in our intercept term, because we have chosen a particular sample size from our total population of n = 186, we have it as 0.058 but if we again picked a different sampling size with different outcome variables, the intercept would surely change, this is due to sampling variability.

### Test statistic
The fourth column in the regression table is of statistics, it includes the t-testing method which tells us what is expected within the null hypothesis.

The *null hypothesis* $H_0$ assumes that the slope is 0 and that there is *no true relationship between consumption value and life expectancy* for all the instructors considered.

The *alternative hypothesis* $H_A$ assumes that the slope is not 0, which means that as the consumption value increases, the life expectancy of people either increases (positive slope) or decreases (negative slope).

Our t-value is -1.986, the negative sign indicates that our hypothesized value is greater than the estimated value or mathematically, the mean of the second variable is greater than the first. But we always use the absolute value of t-value for comparing.


### p-value
The fifth column in the regression table is of p_value, it is the probability that, IF the null hypothesis is true, sampling variations would produce an estimate that is further away from our hypothesized value.

It tells us how likely it is for the effect we have observed of the consumption rate of different consumption type on the life expectancy of people (in other words, the observed fitted slope of b1 = -0.115) to be due to chance. Our p_value is 0, which means that we can reject our null hypothesis.

We can say that there is a relationship between our two variables i.e our alternative hypothesis $H_A$ is true. One thing to be noted here is, that the results of this hypothesis test are only valid if certain *conditions for inference for regression* are met.


### Confidence interval
The last two columns from the right are of our confidence interval. Here, we are 95% confident that our slope lies in the range of -0.229	and -0.001. And for that value for Consumption Tobacco lies in the range of -17.430	and -4.899. Also, we can see that our confidence interval does not contain 0, which means that there is a significant relationship between the outcome and independent variable, thus proving our null hypothesis to be wrong.

But, if we increase our percentage to 99%, there may be a chance of 0 being included. Therefore, the chosen confidence level plays an important role in coming to a result.


3. Residual Analysis 

After forming the equation of regression and understanding what the different values in our regression table indicate, we will be carrying out residual analysis for checking if the four conditions of inference for regression are met for our variables.

```{r}
consumption_interaction_residual <- model_interaction %>% 
  get_regression_points()
consumption_interaction_residual
```

In the above table, the LifeExpectancy_hat represents the fitted value y.

```{r}
consumption_interaction_residual %>% 
  mutate(squared_residuals = residual ^ 2) %>% 
  summarise(sumofSquaredResidual = sum(squared_residuals))
```

Now, we proceed to analyze if the four conditions of inferencing on our regression model are valid or not. 

### (1) **L**inearity of relationship

For meeting this condition, we have a look at the scatter for understanding the relationship between life expectancy and consuming value. Since, the data points are scattered and are not escalating with the regression line. Therefore, the *relationship is not linear*. Also, keeping in mind the regression line is weakly sloped since we obtain a low value correlation, i.e. 0.0043.

```{r}
cor(consumption_model$LifeExpectancy, consumption_model$Value)
```

### (2) **I**ndependence of the residuals

The condition of independence of the residual states that the residual between the two variables must be independent of one another. We determine this by forming the residual lag plot.

```{r}
consumption_interaction_residual %>% 
  mutate(lag_residual = lag(residual)) %>% 
  ggplot(aes(lag_residual,residual)) + geom_point()
```

By looking at our scatter plot, because we get a non-random plot i.e most of the residuals are scattering in the right side of the graph, we can infer that the residuals are fairly *dependent on each other* or auto-correlation.


### (3) **N**ormality of the residuals

The condition of normality of the residuals states that if we form a histogram of our residuals, we should get a normal distribution with the center at 0.

```{r}
consumption_interaction_residual %>% 
  ggplot(aes(residual)) + geom_histogram(binwidth = 6, color = "white") + labs(x = "Residual") + ggtitle("Normality of the Residuals")
```

From our histogram, we can see that the long tail is on the left side of the peak thus forming a *negatively skewed model* which makes more positive errors, i.e. residuals > 0. The model overestimates our outcome variable i.e. life expectancy. Although the desired shape is of a bell shape with the data being evenly spread.

### (4) **E**quality of variance of the residuals

The condition of equality of variance of the residuals states that the distribution between the residual and the independent variable should be homoscedastic (scedastic = scatter). To determine this, we form a scatter plot and compare the residuals with the yintercept = 0.


```{r}
consumption_interaction_residual %>% 
  ggplot(aes(Value, residual)) + geom_point() + labs( x = "Consumption Value", title = "Equality of variance of residuals") +  geom_hline(yintercept = 0, col = "purple", size = 1)
```


In our scatter plot, we can see that the residuals do not vary much as the value of our independent variable, house age changes linearly. Thus we can infer that our model produces an accurate result. We can say that the distribution is homoscedastic.

From our above analysis we can say that:

  1. Linearity of relationship between variables: No
  2. Independence of residuals: No
  3. Normality of residuals: Negatively Skew
  4. Equality of variance: Yes
  
The condition of linearity of relationship between variables was violated. A solution could be to form a lag model. 

Also, after forming the histogram of our residuals with the binwidth of 6, we witnessed a negatively skewed model and not a bell shaped distribution which violated our normality of residuals condition. This condition can be correctly done by adding more explanatory variables into the model which can be done by following multiple regression.

The results of our confidence intervals and hypothesis tests, proved our null hypothesis to be wrong thus saying that there is a relationship existing between our life expectancy and consumption value. As people consume more alcohol, their life expectancy decreases but when people consume tobacco, or smoke regularly, their life expectancy increases but then decreases, in other words, we could conclude that their life expectancy decreased slowly. But since, not all the conditions of linear regression are met, we need to improve the model by experimenting with different techniques.

  
<p id="question"> **Sub Research Question 03: Does exercising and eating healthy food increase the chances of a person living longer?** </p>

For our last research question, we are going to be understanding if a person stays healthy and fit, does it increase his or her chances of living longer. 


Again, our first step would be to comapare the raw data values in our exercise_model dataframe formed by selecting the necessary columns from df. 

```{r}
exercise_model <- df %>% 
  select(Country, Male,Female,MeanBMI) %>% 
  pivot_longer(Male:Female, names_to = "Gender", values_to = "LifeExpectancy")
exercise_model
```

Here, *Country* is the list of the countries for which we have the data available,
      *Life Expectancy* is our *outcome variable y*, which is a numerical variable. It is listed for each Male and Female,
      *Gender* is our *explanatory variable x<sub>2<sub>*, which is our categorical variable,
      *MeanBMI* is our *explanatory variable x<sub>1<sub>*, a numerical variable. It is the average BMI of both Male and Female. 

```{r}
range(exercise_model$LifeExpectancy)
range(exercise_model$MeanBMI)

exercise_model %>% skim()

```

The *Life Expectancy* ranges from minimum 32 years to maximum 84 years and the *Mean BMI* ranges from minimum 20.9 to maximum 31.9. From the range of Mean BMI, we can say that most of the people are fit and are regular exercising group of people.


```{r}
exercise_model %>% 
  slice_sample(n = 5)

exercise_model %>% 
  get_correlation(LifeExpectancy ~ MeanBMI)
```

By selecting for random 5 rows from exercise_model, we can measure the different chances of the raw data values from our dataframe.

Our *correlation coefficient* value is 0.504. The positive value tells us that as the  *MeanBMI* increases the  *LifeExpectancy* increases as well. The positive magnitude also tells us that the shape of the regression line will be facing upwards. Additionally, the relationship between the two is *positively strong*. In other words, as more people exercise and take good care of their health, they tend to live longer.


Since, both the Life Expectancy and the Mean BMI are numerical, we are going to plot them in a scatterplot for better visualization. 

```{r}
exercise_model %>% 
  ggplot(aes(MeanBMI,LifeExpectancy, color = Gender)) + geom_point() + 
  labs(x = "Mean Body Mass Index", 
       y = "Life Expectancy",
       title = "Scatterplot of Relationship of Mean BMI and Life Expectancy")
```

We can infer that most of the values of BMI lie between 22.5 and 30.0 and the Life Expectancy age lies between 30 and 80 for both Male and Female. Our assumption about the relationship between the two variables being *strongly positive* can be seen in the graph as the data points are scattered evenly and there's no sign of any noise. One important point to note here, is that at BMI of 27 and a life expectancy of 71, there seems to be an overlapping of data points which we can further analyze by using geom_jitter().


```{r}
exercise_model %>% 
  ggplot(aes(MeanBMI,LifeExpectancy, color = Gender)) + geom_jitter() + 
  labs(x = "Mean Body Mass Index", 
       y = "Life Expectancy",
       title = "Scatterplot of Relationship of Mean BMI and Life Expectancy") 
```

By using, geom_jitter(), the overlapping data point become more visible but except that the amount of data points shown earlier are same. 

Next, we add the *regression line* or a best-fitting line, for better visualization of our data points. We use the geom_smooth function to add a smoothing line to see what the trends look like. This is helpful to understand regressions.

```{r}
exercise_model %>% 
  ggplot(aes(MeanBMI,LifeExpectancy, color = Gender)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) + 
  labs( x = "Mean Body Mass Index",
        y = "Life Expectancy",
        title = "Regression model of relationship of BMI and Life Expectancy")
```

From our regression model formed above, we can see that there are two lines which are parallel to each other. Our thought of the direction of the regression line being slantly increasing can be seen in the model as the regression lines for both male and female are positively increasing. It also complies with our correlation value being 0.504 indicating saying that the relationship between *Mean BMI* and *Life Expectancy* is strongly positive.


2. Simple Linear Regression

Now, we compute the linear regression coefficients and form our equation of the best-fitting line. We use the moderndrive package to answer this question.

```{r}
exercise_life_model <- lm(LifeExpectancy ~ MeanBMI + Gender , data = exercise_model)
exercise_life_model %>% 
  get_regression_table()
```


In the estimate column of the above table are the intercepts for the baseline for the "Gender Female Life Expectancy". Here $b_0$ is 20.347 and the single slope for Mean BMI for all genders is $b_1$ is 2.062.

The equation of the regression line for the parallel slopes model is: ${\widehat y} = b_0 + b_1 . x_1 + b_2 . x_2$,
and our indicator function is

1<sub>ismale</sub>(x) = 1 if instructor x is male, 0 otherwise

Our equation of the best-fitting line is:  
${\widehat{LifeExpectancy}} = 20.347 + 2.062 . MeanBMI - 5.566 . 1_{is male}(x)$$

Our equation for the Gender Male is:
Here our indicator function for male is 1, so putting x = 1
${\widehat{LifeExpectancy}} = 20.347 + 2.062 . MeanBMI - 5.566 . 1_{is male}(1)$$
${\widehat{LifeExpectancy}} = (20.347 - 5.566) + 2.062 . MeanBMI$$
${\widehat{LifeExpectancy}} = 14.781 + 2.062 . MeanBMI $$

Our equation for Gender Female is:
Here our indicator function for female is 0, so putting x = 0
${\widehat{LifeExpectancy}} = 20.347 + 2.062 . MeanBMI - 5.566 . 1_{is male}(0)$$
${\widehat{LifeExpectancy}} = 20.347 + 2.062 . MeanBMI$$

#Standard Error 

The third column is of our *std_error*. We can expect 0.244 unit of variation in the slope of Mean BMI and a 1.071 unit of variation in the slope for Gender Male. 

Smaller the value of std_error, more closer are the data points to the regression line and thus helping in generating a better estimate. 


#Statistics (t-value)

The fourth column, *statistics* is of t-Value. Here, we form our null and alternative hypothesis. Our t-value for MeanBMI is 8.457, and for Gender Male it is -5.197,  the negative sign indicates that our hypothesized value is greater than the estimated value or mathematically. But we always use the absolute value when comparing t-values. 

The *null hypothesis* $H_0$ assumes that the slope is 0 and that there is *no true relationship between Mean Body Mass Index and Life Expectancy* for both Male and Female. 

The *alternative hypothesis* $H_A$ assumes that the slope is not 0, which means that as the value of Body Mass Index increases, the Life Expectancy of both male and female either increases (positive slope) or decreases (negative slope).

The goal is to determine if the t- value is unusual enough to warrant rejecting our null hypothesis. In order to do that we look into the p_value.


#P_Value

The fifth column in the regression table is of p_value, it is the probability that, IF the null hypothesis is true, sampling variations would produce an estimate that is further away from our hypothesized value.

It tells us how likely it is for the effect we have observed of the house age on the the price of unit area of house (in other words, the observed fitted slope of b1 = 2.062) to be due to chance. Our p_value is 0 for both Mean BMI and Gender Male, which means that we can reject our null hypothesis.

We can say that there is a relationship between our two variable mean BMI and life expectancy i.e our alternative hypothesis HA is true.


#Confidence Interval 

The last two columns from the right are the confidence levels. Here, we are 95% confident that our slope lies in the range of 1.581 and	2.543 and the estimate value of our Gender Male lies in the range of -7.678 and	-3.453. Also, we can see that our confidence interval doesn’t contain 0, which means that there is a significant relationship between the outcome and independent variable, thus proving our null hypothesis to be wrong.

But, if we increase our percentage to 99%, there may be a chance of 0 being included. Therefore, the chosen confidence level plays an important role in coming to a result.


Now, we carry our residual analysis for

```{r}
exercise_parallel_slope <- exercise_life_model %>% 
  get_regression_points()
exercise_parallel_slope
```

In the table above, the Expectancy_hat represents the fitted value y. 

```{r}
exercise_parallel_slope %>% 
  mutate(squared_residual = residual ^ 2) %>% 
  summarise(sumofSquaredResidual = sum(squared_residual))
```

The RSS which stands for Residual Sum of Squares is a measure of understanding the lack of fit (goodness of fit) of a model. Its is done by analysing the residual plots or by computing statistical measures.
The residual sum of squares measures the amount of error remaining between the regression function and the data set which in our case is 9757.574.

### (1) **L**inearity of relationship

```{r}
cor(exercise_model$LifeExpectancy, exercise_model$MeanBMI)
```
By analyzing the scatterplot, the relationship between the Mean BMI and Life Expectancy for male and female are not linear.  But the relationship is strong since the correlation coefficient value between the our independent and dependent variable is 0.504. 


### (2) **I**ndependence of the residuals

The condition of independence of the residual states that the residual between the two variables must be independent of one another. We determine this by forming the residual lag plot. 

```{r}
exercise_parallel_slope %>% 
  mutate(lag_residual = lag(residual)) %>% 
  ggplot(aes(lag_residual, residual)) + geom_point()
```

By looking at our scatterplot, because we get a random plot i.e. the residuals are randomly scattered but we can see some residuals being close to each other. Thus we can say that *the residuals are somewhat independent* on each other. 


### (3) **N**ormality of the residuals

The condition of normality of the residuals states that if we form a histogram of our residuals, we should get a normal distribution with the center at 0.

```{r}
exercise_parallel_slope %>% 
  ggplot(aes(residual)) + geom_histogram(binwidth = 4, color = "white") + labs(x = "Residuals", title = "Normality of Residuals")
```

From our histogram, when we form our bars with a binwidth of 4, we can see that the data is somewhat normally distributed but since the the long tail is on the left side of the peak, it makes it a *negatively skew*. Which means that the regression model makes more positive errors i.e. residuals > 0. So, the model underestimates the outcome variable. 

```{r}
exercise_parallel_slope %>% 
  ggplot(aes(residual)) + geom_histogram(binwidth = 8, color = "white") + labs(x = "Residuals", title = "Normality of Residuals") 
```


And when, we form our histogram with a bandwidth of 8, we see that the condition for negatively skewed still prevails. Therefore, we will be going forward with the model being *negatively skewed*. 

### (4) **E**quality of variance of the residuals

The condition of equality of variance of the residuals states that the distribution between the residual and the independent variable should be homoscedastic (scedastic = scatter). To determine this, we form a scatter plot and compare the residuals with the yintercept = 0.

```{r}
exercise_parallel_slope %>% 
  ggplot(aes(MeanBMI, residual)) + geom_point() + geom_hline(yintercept = 0, col = "purple", size = 1) + ggtitle("Equality of variance of the residuals")
```

In our scatter plot, we can see that the residuals do not vary much as the value of our independent variable Mean BMI increases. Thus, we can infer that the model produces an accurate result. We can say that the distribution is *homoscedastic*.


From our above analysis, we can say that:

1. Linearity of relationship between variables: No
2. Independence of residuals: Somewhat
3. Normality of residuals: Negatively Skewed 
4. Equality of variance: Yes

The condition of linearity of relationship between variables was violated,in order to solve this we can transform the model into a log transform. 

After forming our histogram of our residuals with the bandwidth of 8, we witnessed a somewhat of a bell shaped being formed but since the negatively skewed model and not a bell shaped distribution which violated our normality of residuals condition. This condition can be corrected by adding more explanatory variables into the model.

The results of our confidence intervals and hypothesis tests, proved our null hypothesis was wrong and finalizing that there was a relationship existing between our independent and outcome variables. But since, not all of the conditions have been met, we need to improve our model by experimenting with different techniques. 
______________________________________________________________________________________________________________________________________________

Following are the data sources used to gather our data:

•	Life expectancy at birth  (HTML)
https://en.wikipedia.org/wiki/List_of_countries_by_life_expectancy  

•	Alcohol consumption (CSV)
https://worldpopulationreview.com/country-rankings/alcohol-consumption-by-country  

•	Smoking rate (CSV)
https://worldpopulationreview.com/country-rankings/smoking-rates-by-country 

•	BMI (SQL)
https://en.wikipedia.org/wiki/List_of_countries_by_body_mass_index

•	Literacy Percentage (SQL)

https://data.worldbank.org/indicator/SE.ADT.LITR.ZS 

Conclusion

After performing separate exploratory analysis of our raw data, and understand their relationship between the independent and outcome variable by conducting linear and multiple analysis. For each of our research and sub research question we can conclude as follows: 
The variables having a strong relationship with the Life Expectancy are Literacy Percentage and Mean Body Mass Index. Both having a relative positive impact i.e. as the more countries become literate, their chances of living for long increases. Additionally, if more countries follow a healthy diet and exercised often, then their life expectancy rate would be higher as well. We could infer this not only by the correlation value but also by forming regression models and understand their variance. 

Furthermore, for our case study of understanding how consumption of alcohol and smoking regularly effected a country's life expectancy, we conducted multiple regression. It was interesting to see the regression line for consuming tobacco having a straight increase while for alcohol a straight decrease. So, we can conclude by consuming alcohol, the life expectancy decreases but when consuming tobacco, the life expectancy of a country experiences an increase. Which when compared generally, both the factors should decrease the tendency of living for an individual.

Recommendation 

From our results obtained above, we can recommend the strong associated factors with Life Expectancy being Body Mass Index, Literacy Percentage and the consumption of Alcohol and/or Smoking. Although, not all our conditions were met when inferencing our regression model, it would be recommended to imply techniques to improve our models so as to generate more accurate observations. Although it was interesting to calculate results, but by including more observations in the dataframe and analyzing would result with more accuracy. 